Data Split
	•	We used an 80/20 split for training and testing datasets.
	•	Training Set (80%): Used for model fitting and learning patterns.
	•	Testing Set (20%): Held out for unbiased evaluation of model performance.
	•	In addition, 5-fold cross-validation was applied during training to ensure robustness and reduce variance in results.

⸻

Preprocessing
	•	Data Cleaning: Removed duplicates, handled missing values using imputation (mean/median for numerical, mode for categorical), and standardized inconsistent country/partner names.
	•	Feature Engineering:
	•	Created lag features (e.g., previous year’s unemployment/trade values).
	•	Built ratios such as trade dependency index and exports-to-GDP ratio.
	•	Aggregated bilateral trade values by year to country-level indicators.
	•	Normalization/Scaling: Numerical features were standardized (z-score) to stabilize training.
	•	Encoding: Categorical variables (e.g., country names, regions) were one-hot encoded.

⸻

Model Architecture
	•	We experimented with multiple models depending on the question/task:
	•	Regression Models: Linear Regression, Ridge, Lasso, and ElasticNet for economic indicators (GDP, unemployment).
	•	Tree-Based Models: Decision Trees, Random Forest, and XGBoost for handling non-linear trade dependency and cascading effects.
	•	Time-Series Models: LSTM (Long Short-Term Memory networks) for sequential forecasting tasks (e.g., projecting unemployment to 2030).
	•	Final model choice depended on the evaluation metric:
	•	XGBoost was preferred for structured tabular data due to its robustness to missing values and strong performance.
	•	LSTM was applied to long-term forecasting problems (e.g., youth unemployment projections).

⸻

Hyperparameter Tuning
	•	RandomizedSearchCV and GridSearchCV were used for hyperparameter optimization.
	•	Key tuned parameters included:
	•	XGBoost:
	•	n_estimators = 300
	•	max_depth = 6
	•	learning_rate = 0.05
	•	subsample = 0.8
	•	colsample_bytree = 0.8
	•	Random Forest:
	•	n_estimators = 200
	•	max_depth = None (full growth allowed)
	•	LSTM:
	•	Hidden units = 64
	•	Dropout = 0.2
	•	Optimizer = Adam (lr=0.001)
	•	Final models were selected based on RMSE (Root Mean Squared Error) for regression tasks and MAPE (Mean Absolute Percentage Error) for forecasts.
